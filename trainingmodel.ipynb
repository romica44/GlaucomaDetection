{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOHwSjF8wNdb92TdnrTjxqQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/romica44/GlaucomaDetection/blob/main/trainingmodel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "DETECCION TEMPRANA DE GLAUCOMA"
      ],
      "metadata": {
        "id": "280ENYFYaAj7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oRaf6HX9Z5hM"
      },
      "outputs": [],
      "source": [
        "# Librerías necesarias\n",
        "from PIL import Image\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Adecuar las imagenes antes de entrenar el algoritmo para que todas tengan el mismo tamaño y color\n",
        "\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "path_training = \"http://drive.google.com/drive/folders/1rWPaxRdd0bNlX5-9wI_vGiZb6eCHIE7X?usp=sharing\"\n",
        "\n",
        "path_validation = \"http://drive.google.com/drive/folders/1wEg0RnxZvs3cj3hLHCvBqPtU-onyGYUF?usp=sharing\"\n"
      ],
      "metadata": {
        "id": "XZRx-MbpaJN6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Configurar el generador de datos de imágenes\n",
        "datagen = ImageDataGenerator(\n",
        "    rescale=1./255,  # Normalizar los valores de píxeles a 0-1\n",
        "    validation_split=0.2,  # Separar el 20% de las imágenes para validación\n",
        ")\n"
      ],
      "metadata": {
        "id": "7_9Hls9saM5Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Cargar las imágenes del directorio 'path' y redimensionar a 256x256 píxeles\n",
        "train_ds = datagen.flow_from_directory(\n",
        "    path_training,\n",
        "    target_size=(256, 256),\n",
        "    batch_size=32,\n",
        "    subset='training',\n",
        ")\n"
      ],
      "metadata": {
        "id": "w5RN0yvZaPJS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Cargar las imágenes del directorio 'path' y redimensionar a 256x256 píxeles\n",
        "val_ds = datagen.flow_from_directory(\n",
        "    path_validation,\n",
        "    target_size=(256, 256),\n",
        "    batch_size=32,\n",
        "    subset='validation',\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "5qUimk4TaSeH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Se carga el conjunto de datos y se divide en un conjunto de entrenamiento y otro de prueba\n",
        "\n",
        "# import matplotlib.pyplot as plt\n",
        "# from matplotlib.image import imread\n",
        "\n",
        "# # Ruta de la imagen\n",
        "# img_path = \"/content/drive/My Drive/Glaucoma_Detection/dataset/Fundus_Train_Val_Data/Fundus_Scanes_Sorted/Train/Glaucoma_Positive/036.jpg\"\n",
        "\n",
        "# # Leer la imagen\n",
        "# img = imread(img_path)\n",
        "\n",
        "# # Mostrar la imagen\n",
        "# plt.imshow(img)\n",
        "# plt.axis('off')\n",
        "# plt.show()\n",
        "\n",
        "\n",
        "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    path_training,\n",
        "    validation_split=0.2,\n",
        "    subset=\"training\",\n",
        "    seed=123,\n",
        "    image_size=(256, 256),\n",
        "    batch_size=32)\n",
        "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    path_validation,\n",
        "    validation_split=0.2,\n",
        "    subset=\"validation\",\n",
        "    seed=123,\n",
        "    image_size=(256, 256),\n",
        "    batch_size=32)\n",
        "\n"
      ],
      "metadata": {
        "id": "HW3zK_1kaVDY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocesamiento de datos: En este ejemplo, normalizamos los píxeles de las imágenes y las escalamos a un rango de 0 a 1:\n",
        "normalization_layer = tf.keras.layers.experimental.preprocessing.Rescaling(\n",
        "    1./255)\n",
        "train_ds = train_ds.map(lambda x, y: (normalization_layer(x), y))\n",
        "val_ds = val_ds.map(lambda x, y: (normalization_layer(x), y))\n"
      ],
      "metadata": {
        "id": "1y46Whh8aa4d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Modelo de aprendizaje automático elegico CNN (red neuronal convolucional)\n",
        "model = keras.Sequential([\n",
        "    layers.Conv2D(32, 3, activation='relu'),\n",
        "    layers.MaxPooling2D(),\n",
        "    layers.Conv2D(64, 3, activation='relu'),\n",
        "    layers.MaxPooling2D(),\n",
        "    layers.Conv2D(128, 3, activation='relu'),\n",
        "    layers.MaxPooling2D(),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dense(num_classes)\n",
        "])\n",
        "\n"
      ],
      "metadata": {
        "id": "sKnomkLGadyp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Entrenar el modelo: Utilizamos la función compile para configurar el modelo con una función de pérdida, un optimizador y una métrica de evaluación:\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=['accuracy'])\n",
        "\n"
      ],
      "metadata": {
        "id": "ntSmMQsdagj1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# A continuación, entrenamos el modelo utilizando el conjunto de datos de entrenamiento:\n",
        "history = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=10\n",
        ")\n"
      ],
      "metadata": {
        "id": "SnwmyEjXalDr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluar el modelo: Finalmente, podemos evaluar la precisión del modelo utilizando el conjunto de datos de prueba:\n",
        "test_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    \"path/to/dataset\",\n",
        "    validation_split=None,\n",
        "    subset=\"testing\",\n",
        "    seed=123,\n",
        "    image_size=(256, 256),\n",
        "    batch_size=32)\n",
        "test_ds = test_ds.map(lambda x, y: (normalization_layer(x), y))\n",
        "test_loss, test_acc = model.evaluate(test_ds)\n",
        "print(\"Test accuracy:\", test_acc)\n",
        "\n"
      ],
      "metadata": {
        "id": "HfmjPGpFanHu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}